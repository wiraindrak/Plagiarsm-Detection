Doesn't put me to sleep -- Jay Mathews
March 15, 2008
You can lead a horse to water ...
Philadelphia Inquirer, Bear in mind that in 2005, only 15.8% of black 11th graders in Philadelphia performed at the proficient level or above on the state math test. This placed them 2.61 standard deviations below the mean pass rate of 52.8% in Pennsylvania. This places these students below the first percentile. See . I guess they haven't found the right learning style for these students yet.
A presidential panel said yesterday that America's math education system was "broken," and it called on schools to ensure children from preschool to middle school master key skills. ... F. Joseph Merlino, project director for the Math Science Partnership of Greater Philadelphia, which runs a research program involving 125 schools in 46 school districts, said that while he agreed with the finding that "you can't teach so many topics that you aren't able to get into depth," he disagreed with the report's focus on improving algebra instruction as central to better math education for all students. . (emphasis added)
He said he favored tailoring math instruction to the learning styles of students more than the report does
Statistical Illiteracy
Comparing poverty effects and school/teacher quality effects cross-nationally is fraught with problems. Each country defines poverty levels differently and I'm not sure anyone has devised a methodology for rating school/teachers that is objective, comports with reality, and has predictive value. The OECD botched-up their analysis of SES effects for the 2003 PISA. No doubt this report forms the basis of Professor Rotberg's dubious comparison. Fig 4.8 (p. 176) of this is a pretty graph of the relationship between math performance and SES of OECD countries. See the pretty trend line? Notice how the data is a poor fit to the trend line? Notice how the best of the low-SES students outperformed the worst of the high-SES students? If you turn to Table 4.3b, col. 3, p. 398, Annex B1 you see that the OECD Avg R square is only 17.9%. This means that only 17.9% of the variance in student performance is accounted for in the variance of SES. Notice how I was careful to avoid an implication of causality, something that the OECD and Professor Rotberg take a rather cavalier attitude toward. This means that 82.1% of the variance in student performance is not accounted for by the variance in SES. Other factors, such as school quality, account for this remaining 82.1% The OECD tried to quantify school effects on student performance. Somewhat . (I challenge anyone to read Fig. 3.6 without giggling.) It may be the case that school effects do account for less than poverty effects. Poverty effects account for less than 18%. It could be that school effects account for even less. Certainly, most schools are clueless when it comes to educating lower-SES students. But that is not to say that the few highly effective schools have a greater effect. The data is few and far between. According to Rotberg, the basic problem is poverty which is more reflective of (comparisons involving) testing results. This implies causation when all we have is correlation data. And the correlation between poverty (SES) and testing results is weak (17.9%). Why focus on this small factor when 82% of the variance is attributable to other factors? It's not like anyone's been successful raising student achievement by increasing a student's SES. Two final notes. 1. You can add the term "a lot" to the Downes Lexicon of Misrepresenting Statistics Through Language. Apparently, "a lot" means 22% -- the largest figure I could for childhood poverty in the US (as long as you don't mind non-cash benefits from "Income."). Remember, "often" and "many" are 16% and "a lot" is 22%. We're going to have to make-up some new words to represent truly high levels or frequencies since most of the existing words are quickly being used up for low incidence events. I want to recommend "super duper" for 80% and "shiiiit!" for 90%. 2. According to table 4.3b, the R for SES effects in the U.S. is 23.8%. I the R to be 31.2% for parental education levels in Pennsylvania relative to student performance. I'm still waiting for Stephen Downes to explain to me why Pensylvania's scores don't generalize to the rest of the U.S. (Hat tip to )
Here's a of some dubious reasoning based on unwarranted statistical assumptions. nice example
Iris C. Rotberg, a professor of education policy at George Washington University, said any comparisons based on international tests, such as PISA, would be more reflective of the poverty in a state—or country—than of the quality of its schools or teachers. “Making more comparisons and having more tests won’t solve the basic problem: We have a lot of kids living in poverty,” she said. “Governors can probably predict what their test scores will look like.”
The State of Black Education in Pennsylvania
I've given the mean and weighted mean to show the effect that the Philadelphia School district, which contains a substantial majority of the tested black students, on the data. The numbers in parentheses are the number of tested black students in reading (math numbers were similar). (A quick word on z-scores: the z-scores are given in standard deviation units. Here's a graph you can use to convert z-scores (white) to percentiles (yellow.) So what does the data tell us? These ten school districts are not starved for funding. All but two (Upper Darby and Erie City) have total expenditures above the State mean ($11,322). The weighted mean tells us that the mean black student is enrolled in a school with more expenditures than about 84% of PA school districts. So much for racist funding. I've also given the data for instructional expenditures to give you an idea how much money actually makes its way into the classroom. Unfortunately, Philadelphia's wasteful ways, with instruction accounting for only 36% of total expenditures, brings the weighted mean down to the 36 percentile. This is unfortunate, but as we know from school expenditures are only weakly correlated with student performance (and to make Stephen Downes happy: at least in Pennsylvania for at least the year 2005 (feel free to generalize up to your zone of comfort)). Now let's see how black students are performing in these ten school districts. You better sit down for these numbers. For State mean (All) I'm giving proficiency scores for all students in Pennsylvania, not just black students. Talk about an achievement gap. For math, the achievement gap is -2.57 sd and for reading it is -3.54. This means that the mean black student in these ten school districts is performing at below the first percentile. 99 percent of students in Pennsylvania are outperforming them. (Like I said, I'm suspicious of these scores, but I can't seem to find the error.) These results are fairly consistent across the ten schools. All ten schools perform poorly. The achievement gap tends to be in the neighborhood of a standard deviation, but the gap in most of these schools is double and triple that. I'm not sure exactly what's going on in these schools, but it's a safe bet to say whatever it is it isn't working for these kids. And, bear in mind that about half of the black students have dropped out by this point, so these scores are likely for the top half of the curve.
Is shocking. Appalling. In fact it's so bad, I think I mangled the data somehow. Feel free to perform a reality check. I'm using school district level data from S&P's . Specifically, I'm using the data set from 2005 since it is the latest year containing financial and demographic data. I'm also using scores from the PA State test, the PSSA, for the 11 grade since it represents the end-product of a public school education in Pennsylvania. Pennsylvania has 501 school districts. 127 school districts reported test scores for black students. The remaining 374 school districts presumably had less than 10 black test takers which I believe is the reporting cut-off in Pennsylvania. About 13,206 black 11 graders have scores included in the datas et. Black students are highly concentrated in a handful of school districts. 70% of all black students in reporting school districts are concentrated in the following 10 school districts: Erie City, Harrisburg City, Norristown Area, Penn Hills, Philadelphia City, Pittsburgh, Pocono Mountain, Upper Darby, William Penn, and Woodland Hills. In fact, 2/3 of black students are enrolled in the Philadelphia School District alone. With this in mind, let's look at the data. First let's look at school district expenditures for these ten schools.
The Ongoing Reading First Debacle
During the April 20, 2007 Hearing, IG Higgins confirmed that no one involved in the RF scandal had any actual financial ties or conflicts of interest with respect to any basal reading program. Furthermore, there was never evidence adduced in any IG report that Kame’enui, Simmons, or Vaughn made decisions "key" or otherwise that favored Voyager Passport or any other reading program. How could they have? Less than 10% of the states even specified the actual names of reading programs in their Reading First applications. Even if there was a conflict with the reviewers, they had no way of knowing which programs the states were actually selecting. Moreover, my understanding is that Kame’enui, Simmons, or Vaughn are authors of the DIBELS testing instrument which was subsequently included as a component of Voyager Passport. They were not actual authors of the reading program, nor did they derive profits from the reading program In addition, there is no evidence that Voyager Passport failed to qualify for Reading First funding under the loose "based on SBRR" standards contained in the statute. In fact during the hearing IG Higgins expressly indicated that he had not even looked into this issue. This is an exceedingly slim grounds for a finding of impropriety. To the extent that this is Slavin's first and most prominent point, you can imgaine the strength of his remaining points. This falls under the dubious "appearance of impropriety" standard since no actual harm or foul ever resulted from this activity. Nor was this activity prohibited under statute or any other regulation. As one of the only three reading programs with validated reading research, it's difficult to see how DoE would be prohibited from using Direct Instruction as an exemplary program. Nor is DoE obligated to demonstrate every potential exemplary program. Demonstrating a program does not amount to endorsing it. And there is no evidence adduced that any program was actually endorsed by DOE. According to the IG, there was no actual infraction here, merely the appearance of an infraction. And, even that is a stretch. Reading First is a program directed to at-risk students. DI has more validated reading research with respect to at-risk populations attributed to it than any other reading program, including Slavin's SfA. So, if the Oregon researcher did base their criteria on DI, that would seem to be appropriate. However, based on this it would appear that the DI reading program (Reading Mastery Plus) was not always the highest scoring reading program evaluated by the Oregon Researchers. Take for example the first grade evaluation for discretionary items: Reading Mastery comes in fifth. Success for All appears also to have done well in the evaluation, at least in the early grades. perhaps that accounts for the fact that SfA was on the RF approved list in 28 states. Again, I'm not quite sure what the infraction is here even if what happened is exactly what Slavin claims. Basing the criteria on the most research validated reading program and including that program on a list of programs meeting the criteria does not seem to be prohibited. And, if this amounts to "recommending" or "endorsing" a program then those recommendations and/or endorsements weren't very effective. Less than 3% of states adopted the program despite all the alleged improper recommendations and/or endorsements. I'm not as familiar with the DIBELS part of the scandal, but it appears that Good he received from DIBELS. Moreover, no evidence was ever adduced in any IG report showing that any state was actually steered to adopt DIBELS. I'm not sure about Stern's initial claim; however, there is no evidence of record that DoE steered any state away from SfA. Remember, just because SfA was eligible for RF funding does not mean that they were entitled to it. States had the discretion to exclude any program from RF funding they wanted. DoE could only preclude states from adopting reading programs that did not meet the statutory requirements. DoE could not force a state to include any reading program in RF. In my opinion, excluding any validated reading program, such as DI or SFA from Rf funding, is scandalous. But it's a scandal at the state level, not the federal level. I'm going to skip the last two points since point six makes little sense and I have no way of disproving the assertion point seven. Slavin has had a weak case from day one and nothing in this latest letter changes that fact.
Last week Sol Stern penned an for Fordham detailing the blunders committed in the 2006 Reading First investigations and the media's mis-reporting of same. The article was well written, but didn't contain much new reportage, such as the Slavin/Obey connection. A year and a half ago, I reported much of what is found in Stern's article . In fact, many of Stern's points are identical to points I made. Stern's article was critical of Bob Slavin, creator of Success for All a program that received little Reading First funding. Dean Milot, of , has Slavin's somewhat tepid response. Slavin claims that Stern left out all the juicy parts. I disagree. Stern did include the juicy parts. Stern left out the gristle. But, apparently, Slavin likes gristle. Here are Slavin's arguments: article
EdBizzBuzz posted
1. Stern says nothing about the fact, prominently reported by the Inspector General (IG) and the press, that leaders of the Reading First Technical Assistance Centers, Edward Kame’enui, Deborah Simmons, and Sharon Vaughn, were also authors of the Scott Foresman basal text and authors of Voyager Passport, and yet were making key decisions from the outset that favored basal textbooks and Voyager Passport.
2. Stern says nothing about the fact, reported by the IG, that in the early Reading First Academies, when state leaders were learning how Reading First would operate, they were exposed to speakers representing only Direct Instruction and selected basal textbooks, and were given notebooks full of information on DIBELS and no other assessment.
3. Stern says nothing about the fact, reported by the IG and the press, that the Simmons & Kame’enui “Consumer’s Guide,” based in detail on elements of Direct Instruction, was given by Department officials as the de facto official criterion for “scientifically based research”. The Oregon review of reading programs, based on the guide and carried out in part by University of Oregon researchers who were authors of one of the programs, was frequently recommended by the Department as a list of programs to be used under Reading First.
summary report
4. Stern fails to mention how DIBELS became the de facto national assessment of Reading First in most states, enriching Roland Good and the University of Oregon, one of the Technical Assistance Centers. Under Department funding, Good and Oregon colleagues reviewed a variety of reading measures and gave positive ratings to DIBELS.
actually donated all the royalties
One official, Roland H. Good III, said his company made $1.3 million off a reading test, known as DIBELS, that was endorsed by a Reading First evaluation panel he sat on. Good, who owns half the company, Dynamic Measurement Group, told the committee that he donated royalties from the product to the University of Oregon, where he is an associate professor.
5. Stern claims that schools avoided Success for All (SFA) just because it was too expensive. ... We have a file drawer full of anguished reports from Success for All schools and potential SFA schools all over the U.S. pressured by state or local Reading First officials to avoid SFA because it was inconsistent with Reading First. Many schools refused Reading First funding to adopt or keep Success for All. If the Department did not directly tell state officials to exclude SFA, they did not correct the national perception that SFA did not fit in Reading First.
Theory IV: Do Poor Students Perform Better in School Districts with Educated Parents
Here's the regression results for low-SES students: (The correlation between high-school diplomas and student achievement was similar.) We see that as parental education increases so does student (all) scores. There is a medium sized correlation and fit between the data. This correlation is well known. Things start falling apart, however, when we look at the performance of low-SES students. We should expect to see a lower correlation because we've restricted the range of students. Both the correlation and data fit for low-SES students is low. The trend is not reliable given the low data fit. I've left it in for reference purposes. Whatever is causing the increase in student (all) scores seems to be missing from the low-SES student scores. The magic aura of the kids with highly educated parents does not seem to be rubbing off as well as some have predicted. Few of school districts have low-SES scores above the mean for all students. To the extent that low-SES test scores are slightly higher in school districts with high percentages of highly educated parents, this is most likely attributable to of the low-SES students. We know that SES is positively associated with student performance. We also know that the variance in family income for students eligible for free or reduced lunches is wide and includes about 40% of all students, , most students to the left of the mean. The take away is that just because the correlation between parental education and total student performance is midsized, does not mean that the correlation between low-SES student performance and parental education is also midsized. It's much lower than that. And the student achievement gains we should expect to see by placing low-SES students in higher-SES school districts (as a function of parental education) will likely be low to non-existent. In any event, we should not expect those gains to pull low-SES student achievement up to the mean performance of all children based on this data.
Let's take a look at what the data shows with respect to the performance of low-SES students attending school districts with highly educated parents. The theory is that low-SES students will perform better in these school districts. Educated parents are supposed to value education more highly and care about the education their children are receiving. Apparently, these high-SES districts attract better teachers, have better students with better motivation, have more resources, can pay more attention to the small number of low-SES students, and the like. The children of these highly educated parents are supposed to value their education more highly and, as a result, will perform better. In theory, low-SES students should perform better when in classrooms with these children of the highly educated. Let's see if the data supports this theory. The graph compares student achievement (11 grade combined math and reading) with the percentage of adults with bachelor degrees residing in the school district. There are two populations being compared. The pink squares represent all students. The blue diamonds represent economically disadvantaged students (those qualifying for free or reduced lunch). I added trend lines for both populations and also indicated the mean score of all students (the horizontal red line. Mean = 60.7) . Here's the regression results for all students:
th
R = 0.56 (there is a medium association between parental education (of the community) and the performance of all students)
R = 0.312 (there is a medium fit between the data)2
P = 3.77729 x 10 (the results are statistically significant)-42
R = 0.22 (there is a small association between parental education (of the community) and the performance of low-SES students)
R = 0.048 (there is a low poor between the data)2
P = 0.000003 (the results are statistically significant)
Interpretation